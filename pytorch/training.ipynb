{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "validation_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set, batch_size=4, shuffle=False\n",
    ")\n",
    "\n",
    "# Class labels\n",
    "classes = (\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle Boot\",\n",
    ")\n",
    "\n",
    "# Report split sizes\n",
    "print(\"Training set has {} instances\".format(len(training_set)))\n",
    "print(\"Validation set has {} instances\".format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag  T-shirt/top  T-shirt/top  Sneaker\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print(\"  \".join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2027, 0.7758, 0.6546, 0.1958, 0.5616, 0.3228, 0.5294, 0.6439, 0.4413,\n",
      "         0.8575],\n",
      "        [0.6791, 0.4908, 0.8207, 0.2844, 0.9835, 0.8749, 0.4199, 0.9499, 0.4359,\n",
      "         0.1692],\n",
      "        [0.2053, 0.2992, 0.0452, 0.6575, 0.6060, 0.2871, 0.1260, 0.5735, 0.8795,\n",
      "         0.0504],\n",
      "        [0.8608, 0.8139, 0.2919, 0.9300, 0.1749, 0.4036, 0.3547, 0.0137, 0.2055,\n",
      "         0.9752]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.2622978687286377\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print(\"Total loss for this batch: {}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6427, 0.4298, 0.0531, 0.0839, 0.1472, 0.8676, 0.2368, 0.2266, 0.0930,\n",
      "         0.2592],\n",
      "        [0.5943, 0.3848, 0.5141, 0.8261, 0.0922, 0.7567, 0.2753, 0.0226, 0.2337,\n",
      "         0.1361],\n",
      "        [0.5198, 0.8723, 0.5681, 0.0036, 0.3890, 0.7835, 0.6871, 0.5855, 0.8538,\n",
      "         0.6751],\n",
      "        [0.0535, 0.8885, 0.7345, 0.6023, 0.8336, 0.6804, 0.1691, 0.1500, 0.2333,\n",
      "         0.0490]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.4350783824920654\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print(\"Total loss for this batch: {}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000  # loss per batch\n",
    "            print(\"  batch {} loss: {}\".format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar(f\"Loss/train\", last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 1.71890797868371\n",
      "  batch 2000 loss: 0.8410614807754755\n",
      "  batch 3000 loss: 0.7341669770665467\n",
      "  batch 4000 loss: 0.6507931683054193\n",
      "  batch 5000 loss: 0.5982694115387276\n",
      "  batch 6000 loss: 0.5795191136083159\n",
      "  batch 7000 loss: 0.5587598901305464\n",
      "  batch 8000 loss: 0.5120745292429346\n",
      "  batch 9000 loss: 0.5078222500344854\n",
      "  batch 10000 loss: 0.4637909776595188\n",
      "  batch 11000 loss: 0.4648534555595834\n",
      "  batch 12000 loss: 0.445894681220816\n",
      "  batch 13000 loss: 0.45029449728457255\n",
      "  batch 14000 loss: 0.4205620481822407\n",
      "  batch 15000 loss: 0.43219483105896506\n",
      "LOSS train 0.43219483105896506 valid 0.4624616801738739\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.4190816942504607\n",
      "  batch 2000 loss: 0.42125759257911705\n",
      "  batch 3000 loss: 0.38344478900998363\n",
      "  batch 4000 loss: 0.3982890654265648\n",
      "  batch 5000 loss: 0.37910360554099315\n",
      "  batch 6000 loss: 0.3976631571915932\n",
      "  batch 7000 loss: 0.38256783194339367\n",
      "  batch 8000 loss: 0.369309157315176\n",
      "  batch 9000 loss: 0.3680702534214943\n",
      "  batch 10000 loss: 0.3495983114922419\n",
      "  batch 11000 loss: 0.3713731359151134\n",
      "  batch 12000 loss: 0.34565558457688894\n",
      "  batch 13000 loss: 0.35581261797946356\n",
      "  batch 14000 loss: 0.3358490452132828\n",
      "  batch 15000 loss: 0.3622206840161707\n",
      "LOSS train 0.3622206840161707 valid 0.37543052434921265\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.3424150559982227\n",
      "  batch 2000 loss: 0.332877546220232\n",
      "  batch 3000 loss: 0.32732155472712476\n",
      "  batch 4000 loss: 0.32235826795737377\n",
      "  batch 5000 loss: 0.33845333558737184\n",
      "  batch 6000 loss: 0.3236136619267054\n",
      "  batch 7000 loss: 0.3189331544121669\n",
      "  batch 8000 loss: 0.3302389020587871\n",
      "  batch 9000 loss: 0.3307564376797527\n",
      "  batch 10000 loss: 0.3209757764626411\n",
      "  batch 11000 loss: 0.33103677162740497\n",
      "  batch 12000 loss: 0.3254020429418888\n",
      "  batch 13000 loss: 0.33219014009276\n",
      "  batch 14000 loss: 0.30972594318240226\n",
      "  batch 15000 loss: 0.32545014864905536\n",
      "LOSS train 0.32545014864905536 valid 0.3256344199180603\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.2975227780807836\n",
      "  batch 2000 loss: 0.3366216703563987\n",
      "  batch 3000 loss: 0.29292755181916075\n",
      "  batch 4000 loss: 0.3076336345742311\n",
      "  batch 5000 loss: 0.2991366926938717\n",
      "  batch 6000 loss: 0.2952278020546291\n",
      "  batch 7000 loss: 0.2850997364417999\n",
      "  batch 8000 loss: 0.29706934510264543\n",
      "  batch 9000 loss: 0.28881394201699007\n",
      "  batch 10000 loss: 0.2888835334283722\n",
      "  batch 11000 loss: 0.29900816500729704\n",
      "  batch 12000 loss: 0.3275882575546857\n",
      "  batch 13000 loss: 0.27824069290354964\n",
      "  batch 14000 loss: 0.3070584668958436\n",
      "  batch 15000 loss: 0.2962032337341952\n",
      "LOSS train 0.2962032337341952 valid 0.3303827941417694\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.2851215327882819\n",
      "  batch 2000 loss: 0.276573536117281\n",
      "  batch 3000 loss: 0.2752948229030517\n",
      "  batch 4000 loss: 0.2837077984501811\n",
      "  batch 5000 loss: 0.270312667030933\n",
      "  batch 6000 loss: 0.2750186862064329\n",
      "  batch 7000 loss: 0.2755891343676194\n",
      "  batch 8000 loss: 0.29180126116998145\n",
      "  batch 9000 loss: 0.2997606594602112\n",
      "  batch 10000 loss: 0.28975987542167786\n",
      "  batch 11000 loss: 0.27316545620532634\n",
      "  batch 12000 loss: 0.28062729151034727\n",
      "  batch 13000 loss: 0.2760099354147969\n",
      "  batch 14000 loss: 0.3004897643560107\n",
      "  batch 15000 loss: 0.27934089888104063\n",
      "LOSS train 0.27934089888104063 valid 0.30657699704170227\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(\"logs/fashion_trainer_{}\".format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}:\".format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(\"LOSS train {} valid {}\".format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars(\n",
    "        \"Training vs. Validation Loss\",\n",
    "        {\"Training\": avg_loss, \"Validation\": avg_vloss},\n",
    "        epoch_number + 1,\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = \"model_{}_{}\".format(timestamp, epoch_number)\n",
    "        # make models dir\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        # prepend model to model_path\n",
    "        model_path = os.path.join(\"models\", model_path)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = GarmentClassifier()\n",
    "saved_model.load_state_dict(torch.load(\"models/model_20230715_151041_4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cbook' has no attribute '_rename_parameter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmodels\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattr\u001b[39;00m \u001b[39mimport\u001b[39;00m IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattr\u001b[39;00m \u001b[39mimport\u001b[39;00m visualization \u001b[39mas\u001b[39;00m viz\n",
      "File \u001b[0;32m~/Github/workbench/pytorch/.ml_venv/lib/python3.10/site-packages/captum/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#!/usr/bin/env python3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattr\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mattr\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconcept\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mconcept\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfluence\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39minfluence\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/workbench/pytorch/.ml_venv/lib/python3.10/site-packages/captum/attr/__init__.py:60\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mshapley_value\u001b[39;00m \u001b[39mimport\u001b[39;00m ShapleyValues, ShapleyValueSampling  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     configure_interpretable_embedding_layer,\n\u001b[1;32m     56\u001b[0m     InterpretableEmbeddingBase,\n\u001b[1;32m     57\u001b[0m     remove_interpretable_embedding_layer,\n\u001b[1;32m     58\u001b[0m     TokenReferenceBase,\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m visualization  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattribution\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa  # noqa  # noqa  # noqa  # noqa\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     Attribution,\n\u001b[1;32m     63\u001b[0m     GradientAttribution,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     PerturbationAttribution,\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcaptum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclass_summarizer\u001b[39;00m \u001b[39mimport\u001b[39;00m ClassSummarizer\n",
      "File \u001b[0;32m~/Github/workbench/pytorch/.ml_venv/lib/python3.10/site-packages/captum/attr/_utils/visualization.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfigure\u001b[39;00m \u001b[39mimport\u001b[39;00m Figure\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mimport\u001b[39;00m axis, figure\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmpl_toolkits\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maxes_grid1\u001b[39;00m \u001b[39mimport\u001b[39;00m make_axes_locatable\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m ndarray\n\u001b[1;32m     15\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Github/workbench/pytorch/.ml_venv/lib/python3.10/site-packages/mpl_toolkits/axes_grid1/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m axes_size \u001b[39mas\u001b[39;00m Size\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maxes_divider\u001b[39;00m \u001b[39mimport\u001b[39;00m Divider, SubplotDivider, make_axes_locatable\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maxes_grid\u001b[39;00m \u001b[39mimport\u001b[39;00m Grid, ImageGrid, AxesGrid\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparasite_axes\u001b[39;00m \u001b[39mimport\u001b[39;00m host_subplot, host_axes\n",
      "File \u001b[0;32m~/Github/workbench/pytorch/.ml_venv/lib/python3.10/site-packages/mpl_toolkits/axes_grid1/axes_grid.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m     ax\u001b[39m.\u001b[39maxis[\u001b[39m\"\u001b[39m\u001b[39mbottom\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtoggle(ticklabels\u001b[39m=\u001b[39mbottom_off, label\u001b[39m=\u001b[39mbottom_off)\n\u001b[1;32m     19\u001b[0m     ax\u001b[39m.\u001b[39maxis[\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtoggle(ticklabels\u001b[39m=\u001b[39mleft_off, label\u001b[39m=\u001b[39mleft_off)\n\u001b[0;32m---> 22\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCbarAxesBase\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, orientation, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morientation \u001b[39m=\u001b[39m orientation\n",
      "File \u001b[0;32m~/Github/workbench/pytorch/.ml_venv/lib/python3.10/site-packages/mpl_toolkits/axes_grid1/axes_grid.py:29\u001b[0m, in \u001b[0;36mCbarAxesBase\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_locator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# deprecated.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 29\u001b[0m \u001b[39m@cbook\u001b[39m\u001b[39m.\u001b[39;49m_rename_parameter(\u001b[39m\"\u001b[39m\u001b[39m3.2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlocator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mticks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolorbar\u001b[39m(\u001b[39mself\u001b[39m, mappable, \u001b[39m*\u001b[39m, ticks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morientation \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbottom\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     33\u001b[0m         orientation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhorizontal\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Github/workbench/pytorch/.ml_venv/lib/python3.10/site-packages/matplotlib/_api/__init__.py:226\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.cbook' has no attribute '_rename_parameter'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
