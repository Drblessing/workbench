{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Device count: 0\n",
      "Python version: 3.10.9 (v3.10.9:1dd9be6584, Dec  6 2022, 14:37:36) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "Pytorch version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Union, Optional, Dict, Any, Callable, Iterable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PytorchConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_cpu=False,\n",
    "        batch_size: int = 256,\n",
    "        epochs: int = 100,\n",
    "        data_path: str = \"input\",\n",
    "        target: str = \"num_sold\",\n",
    "    ):\n",
    "        self.device = PytorchConfig.get_device(use_cpu)\n",
    "        self.device_count = PytorchConfig.get_device_count()\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.data_path = data_path\n",
    "        self.target = target\n",
    "        PytorchConfig.get_versions()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_device(use_cpu=False):\n",
    "        \"\"\"Gets device for pytorch.\"\"\"\n",
    "        device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        if use_cpu:\n",
    "            device = \"cpu\"\n",
    "        print(f\"Device: {device}\")\n",
    "        return device\n",
    "\n",
    "    @staticmethod\n",
    "    def get_device_count():\n",
    "        \"\"\"Gets device count for pytorch.\"\"\"\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f\"Device count: {device_count}\")\n",
    "        return device_count\n",
    "\n",
    "    @staticmethod\n",
    "    def get_versions():\n",
    "        \"\"\"Prints python and pytorch versions.\"\"\"\n",
    "        print(f\"Python version: {sys.version}\")\n",
    "        print(f\"Pytorch version: {torch.__version__}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_tabular_data(data_path):\n",
    "        \"\"\"Loads tabular data from csv files.\"\"\"\n",
    "        train_path = os.path.join(data_path, \"train.csv\")\n",
    "        test_path = os.path.join(data_path, \"test.csv\")\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        return train_df, test_df\n",
    "\n",
    "    @staticmethod\n",
    "    def process_tabular_data(train_df, test_df, target):\n",
    "        \"\"\"Standard preprocessing for tabular data.\"\"\"\n",
    "\n",
    "        # Remove id column if present\n",
    "        if \"id\" in train_df.columns:\n",
    "            train_df.drop(\"id\", axis=1, inplace=True)\n",
    "        if \"id\" in test_df.columns:\n",
    "            test_df.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "class DataProcessing:\n",
    "    \"\"\"Holds sklearn and pytorch data processing objects.\"\"\"\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, test_df: pd.DataFrame, target: str):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.target = target\n",
    "        self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"Modifiable function to processes data for training.\"\"\"\n",
    "        PytorchConfig.process_tabular_data(self.train_df, self.test_df, self.target)\n",
    "\n",
    "        # Date encoding\n",
    "        self.date_encode(self.train_df, \"date\")\n",
    "        self.date_encode(self.test_df, \"date\")\n",
    "\n",
    "        # One hot encoding\n",
    "        ohe_cols = [\"country\", \"store\", \"product\"]\n",
    "        self.train_df = self.one_hot_encode(self.train_df, ohe_cols)\n",
    "        self.test_df = self.one_hot_encode(self.test_df, ohe_cols)\n",
    "\n",
    "        # Train test split\n",
    "        self.train_val_split()\n",
    "\n",
    "        # Get all cols except target\n",
    "        standardize_cols = [\"year\", \"month\", \"day\", \"dayofweek\", \"days_since_min_date\"]\n",
    "\n",
    "        # Standardize feature cols\n",
    "        self.standardize_feature_cols(self.train_df, standardize_cols)\n",
    "\n",
    "        # Standardize target col\n",
    "        self.standardize_target_col(self.train_df, self.target)\n",
    "\n",
    "    def date_encode(self, df: pd.DataFrame, date_col: str):\n",
    "        \"\"\"Encodes date columns.\"\"\"\n",
    "\n",
    "        # Turn date columns into datetime objects\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "        # Create year, month, day, and day of week columns\n",
    "        df[\"year\"] = df[date_col].dt.year\n",
    "        df[\"month\"] = df[date_col].dt.month\n",
    "        df[\"day\"] = df[date_col].dt.day\n",
    "        df[\"dayofweek\"] = df[date_col].dt.dayofweek\n",
    "\n",
    "        # Create days since min date column\n",
    "        min_date = df[date_col].min()\n",
    "        df[\"days_since_min_date\"] = (df[date_col] - min_date).dt.days\n",
    "\n",
    "        # Drop original date column\n",
    "        df.drop(date_col, axis=1, inplace=True)\n",
    "\n",
    "    def one_hot_encode(self, df: pd.DataFrame, cols: List[str]):\n",
    "        \"\"\"One hot encode cols.\"\"\"\n",
    "        df = pd.get_dummies(df, columns=cols, dtype=np.float32)\n",
    "        return df\n",
    "\n",
    "    def train_val_split(self, val_size: float = 0.2):\n",
    "        \"\"\"Splits data into train and validation sets.\"\"\"\n",
    "        self.train_df, self.val_df = train_test_split(\n",
    "            self.train_df, test_size=val_size, random_state=42\n",
    "        )\n",
    "\n",
    "        # Reset index\n",
    "        self.train_df.reset_index(drop=True, inplace=True)\n",
    "        self.val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def standardize_feature_cols(self, df: pd.DataFrame, feature_cols: List[str]):\n",
    "        \"\"\"Standardizes feature cols and saves scaler. Input shoudl be train df.\"\"\"\n",
    "\n",
    "        # Init scaler\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # Train scaler\n",
    "        self.feature_scaler.fit(df[feature_cols])\n",
    "\n",
    "        # Standardize feature cols\n",
    "        df[feature_cols] = self.feature_scaler.transform(df[feature_cols])\n",
    "\n",
    "        # Standardize val and test df\n",
    "        self.val_df[feature_cols] = self.feature_scaler.transform(\n",
    "            self.val_df[feature_cols]\n",
    "        )\n",
    "\n",
    "        self.test_df[feature_cols] = self.feature_scaler.transform(\n",
    "            self.test_df[feature_cols]\n",
    "        )\n",
    "\n",
    "    def standardize_target_col(self, df: pd.DataFrame, target_col: str):\n",
    "        \"\"\"Standardizes target col for large target and saves scaler. Need to\n",
    "        inverse predictions after training. Input should be train df.\"\"\"\n",
    "\n",
    "        # Init scaler\n",
    "        self.target_scaler = StandardScaler()\n",
    "\n",
    "        # Train scaler\n",
    "        self.target_scaler.fit(df[[target_col]])\n",
    "\n",
    "        # Standardize target col\n",
    "        df[[target_col]] = self.target_scaler.transform(df[[target_col]])\n",
    "\n",
    "        # Standardize val df\n",
    "        self.val_df[[target_col]] = self.target_scaler.transform(\n",
    "            self.val_df[[target_col]]\n",
    "        )\n",
    "\n",
    "        # Save inverse scaler\n",
    "        self.inverse_target_scaler = self.target_scaler.inverse_transform\n",
    "\n",
    "    def to_pytorch_dataset(self):\n",
    "        \"\"\"Turns data into pytorch dataset using TensorDataset.\"\"\"\n",
    "\n",
    "        # Trun train,val and test df into float32\n",
    "        self.train_df = self.train_df.astype(np.float32)\n",
    "        self.val_df = self.val_df.astype(np.float32)\n",
    "        self.test_df = self.test_df.astype(np.float32)\n",
    "\n",
    "        # Unsqueezes target col\n",
    "        target_train = torch.tensor(self.train_df[self.target].values).unsqueeze(1)\n",
    "        target_val = torch.tensor(self.val_df[self.target].values).unsqueeze(1)\n",
    "\n",
    "        train_ds = TensorDataset(\n",
    "            torch.tensor(self.train_df.drop(self.target, axis=1).values),\n",
    "            target_train,\n",
    "        )\n",
    "\n",
    "        val_ds = TensorDataset(\n",
    "            torch.tensor(self.val_df.drop(self.target, axis=1).values),\n",
    "            target_val,\n",
    "        )\n",
    "\n",
    "        test_ds = TensorDataset(\n",
    "            torch.tensor(self.test_df.values), torch.zeros(self.test_df.shape[0])\n",
    "        )\n",
    "\n",
    "        return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_PATH = \"input\"\n",
    "TARGET = \"num_sold\"\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 100\n",
    "pytorch_config = PytorchConfig(\n",
    "    use_cpu=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    data_path=DATA_PATH,\n",
    "    target=TARGET,\n",
    ")\n",
    "\n",
    "\n",
    "train_df, test_df = pytorch_config.load_tabular_data(DATA_PATH)\n",
    "dp = DataProcessing(train_df, test_df, TARGET)\n",
    "train_ds, val_ds, test_ds = dp.to_pytorch_dataset()\n",
    "\n",
    "\n",
    "# Create a PyTorch data loader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds, batch_size=pytorch_config.batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_ds, batch_size=pytorch_config.batch_size)\n",
    "test_loader = DataLoader(test_ds, batch_size=len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Training loss: 0.1294 | Validation loss: 0.0334\n",
      "Epoch 2/100 | Training loss: 0.0411 | Validation loss: 0.0253\n",
      "Epoch 3/100 | Training loss: 0.0319 | Validation loss: 0.0207\n",
      "Epoch 4/100 | Training loss: 0.0271 | Validation loss: 0.0223\n",
      "Epoch 5/100 | Training loss: 0.0230 | Validation loss: 0.0172\n",
      "Epoch 6/100 | Training loss: 0.0215 | Validation loss: 0.0174\n",
      "Epoch 7/100 | Training loss: 0.0193 | Validation loss: 0.0114\n",
      "Epoch 8/100 | Training loss: 0.0179 | Validation loss: 0.0112\n",
      "Epoch 9/100 | Training loss: 0.0177 | Validation loss: 0.0116\n",
      "Epoch 10/100 | Training loss: 0.0166 | Validation loss: 0.0104\n",
      "Epoch 11/100 | Training loss: 0.0170 | Validation loss: 0.0125\n",
      "Epoch 12/100 | Training loss: 0.0161 | Validation loss: 0.0102\n",
      "Epoch 13/100 | Training loss: 0.0157 | Validation loss: 0.0118\n",
      "Epoch 14/100 | Training loss: 0.0148 | Validation loss: 0.0095\n",
      "Epoch 15/100 | Training loss: 0.0163 | Validation loss: 0.0104\n",
      "Epoch 16/100 | Training loss: 0.0147 | Validation loss: 0.0096\n",
      "Epoch 17/100 | Training loss: 0.0140 | Validation loss: 0.0100\n",
      "Epoch 18/100 | Training loss: 0.0146 | Validation loss: 0.0106\n",
      "Epoch 19/100 | Training loss: 0.0145 | Validation loss: 0.0102\n",
      "Epoch 20/100 | Training loss: 0.0141 | Validation loss: 0.0087\n",
      "Epoch 21/100 | Training loss: 0.0145 | Validation loss: 0.0125\n",
      "Epoch 22/100 | Training loss: 0.0154 | Validation loss: 0.0107\n",
      "Epoch 23/100 | Training loss: 0.0135 | Validation loss: 0.0090\n",
      "Epoch 24/100 | Training loss: 0.0131 | Validation loss: 0.0088\n",
      "Epoch 25/100 | Training loss: 0.0142 | Validation loss: 0.0082\n",
      "Epoch 26/100 | Training loss: 0.0130 | Validation loss: 0.0098\n",
      "Epoch 27/100 | Training loss: 0.0138 | Validation loss: 0.0108\n",
      "Epoch 28/100 | Training loss: 0.0133 | Validation loss: 0.0080\n",
      "Epoch 29/100 | Training loss: 0.0134 | Validation loss: 0.0109\n",
      "Epoch 30/100 | Training loss: 0.0126 | Validation loss: 0.0080\n",
      "Epoch 31/100 | Training loss: 0.0125 | Validation loss: 0.0107\n",
      "Epoch 32/100 | Training loss: 0.0133 | Validation loss: 0.0083\n",
      "Epoch 33/100 | Training loss: 0.0127 | Validation loss: 0.0086\n",
      "Epoch 34/100 | Training loss: 0.0133 | Validation loss: 0.0089\n",
      "Epoch 35/100 | Training loss: 0.0124 | Validation loss: 0.0079\n",
      "Epoch 36/100 | Training loss: 0.0127 | Validation loss: 0.0078\n",
      "Epoch 37/100 | Training loss: 0.0129 | Validation loss: 0.0086\n",
      "Epoch 38/100 | Training loss: 0.0138 | Validation loss: 0.0111\n",
      "Epoch 39/100 | Training loss: 0.0126 | Validation loss: 0.0141\n",
      "Epoch 40/100 | Training loss: 0.0128 | Validation loss: 0.0087\n",
      "Epoch 41/100 | Training loss: 0.0119 | Validation loss: 0.0077\n",
      "Epoch 42/100 | Training loss: 0.0122 | Validation loss: 0.0080\n",
      "Epoch 43/100 | Training loss: 0.0124 | Validation loss: 0.0078\n",
      "Epoch 44/100 | Training loss: 0.0121 | Validation loss: 0.0081\n",
      "Epoch 45/100 | Training loss: 0.0124 | Validation loss: 0.0081\n",
      "Epoch 46/100 | Training loss: 0.0118 | Validation loss: 0.0088\n",
      "Epoch 47/100 | Training loss: 0.0123 | Validation loss: 0.0078\n",
      "Epoch 48/100 | Training loss: 0.0126 | Validation loss: 0.0112\n",
      "Epoch 49/100 | Training loss: 0.0123 | Validation loss: 0.0073\n",
      "Epoch 50/100 | Training loss: 0.0118 | Validation loss: 0.0076\n",
      "Epoch 51/100 | Training loss: 0.0121 | Validation loss: 0.0089\n",
      "Epoch 52/100 | Training loss: 0.0120 | Validation loss: 0.0083\n",
      "Epoch 53/100 | Training loss: 0.0116 | Validation loss: 0.0077\n",
      "Epoch 54/100 | Training loss: 0.0119 | Validation loss: 0.0074\n",
      "Epoch 55/100 | Training loss: 0.0118 | Validation loss: 0.0075\n",
      "Epoch 56/100 | Training loss: 0.0118 | Validation loss: 0.0092\n",
      "Epoch 57/100 | Training loss: 0.0112 | Validation loss: 0.0072\n",
      "Epoch 58/100 | Training loss: 0.0116 | Validation loss: 0.0082\n",
      "Epoch 59/100 | Training loss: 0.0115 | Validation loss: 0.0075\n",
      "Epoch 60/100 | Training loss: 0.0119 | Validation loss: 0.0092\n",
      "Epoch 61/100 | Training loss: 0.0113 | Validation loss: 0.0087\n",
      "Epoch 62/100 | Training loss: 0.0113 | Validation loss: 0.0095\n",
      "Epoch 63/100 | Training loss: 0.0113 | Validation loss: 0.0073\n",
      "Epoch 64/100 | Training loss: 0.0109 | Validation loss: 0.0096\n",
      "Epoch 65/100 | Training loss: 0.0130 | Validation loss: 0.0090\n",
      "Epoch 66/100 | Training loss: 0.0108 | Validation loss: 0.0079\n",
      "Epoch 67/100 | Training loss: 0.0115 | Validation loss: 0.0081\n",
      "Epoch 68/100 | Training loss: 0.0113 | Validation loss: 0.0085\n",
      "Epoch 69/100 | Training loss: 0.0120 | Validation loss: 0.0078\n",
      "Epoch 70/100 | Training loss: 0.0118 | Validation loss: 0.0072\n",
      "Epoch 71/100 | Training loss: 0.0110 | Validation loss: 0.0082\n",
      "Epoch 72/100 | Training loss: 0.0113 | Validation loss: 0.0071\n",
      "Epoch 73/100 | Training loss: 0.0107 | Validation loss: 0.0074\n",
      "Epoch 74/100 | Training loss: 0.0110 | Validation loss: 0.0082\n",
      "Epoch 75/100 | Training loss: 0.0113 | Validation loss: 0.0098\n",
      "Epoch 76/100 | Training loss: 0.0113 | Validation loss: 0.0072\n",
      "Epoch 77/100 | Training loss: 0.0109 | Validation loss: 0.0080\n",
      "Epoch 78/100 | Training loss: 0.0112 | Validation loss: 0.0072\n",
      "Epoch 79/100 | Training loss: 0.0109 | Validation loss: 0.0073\n",
      "Epoch 80/100 | Training loss: 0.0107 | Validation loss: 0.0078\n",
      "Epoch 81/100 | Training loss: 0.0111 | Validation loss: 0.0073\n",
      "Epoch 82/100 | Training loss: 0.0109 | Validation loss: 0.0075\n",
      "Epoch 83/100 | Training loss: 0.0115 | Validation loss: 0.0087\n",
      "Epoch 84/100 | Training loss: 0.0109 | Validation loss: 0.0077\n",
      "Epoch 85/100 | Training loss: 0.0112 | Validation loss: 0.0071\n",
      "Epoch 86/100 | Training loss: 0.0105 | Validation loss: 0.0076\n",
      "Epoch 87/100 | Training loss: 0.0108 | Validation loss: 0.0088\n",
      "Epoch 88/100 | Training loss: 0.0104 | Validation loss: 0.0075\n",
      "Epoch 89/100 | Training loss: 0.0106 | Validation loss: 0.0089\n",
      "Epoch 90/100 | Training loss: 0.0109 | Validation loss: 0.0087\n",
      "Epoch 91/100 | Training loss: 0.0110 | Validation loss: 0.0081\n",
      "Epoch 92/100 | Training loss: 0.0112 | Validation loss: 0.0071\n",
      "Epoch 93/100 | Training loss: 0.0113 | Validation loss: 0.0097\n",
      "Epoch 94/100 | Training loss: 0.0113 | Validation loss: 0.0081\n",
      "Epoch 95/100 | Training loss: 0.0108 | Validation loss: 0.0111\n",
      "Epoch 96/100 | Training loss: 0.0105 | Validation loss: 0.0070\n",
      "Epoch 97/100 | Training loss: 0.0108 | Validation loss: 0.0073\n",
      "Epoch 98/100 | Training loss: 0.0108 | Validation loss: 0.0070\n",
      "Epoch 99/100 | Training loss: 0.0104 | Validation loss: 0.0074\n",
      "Epoch 100/100 | Training loss: 0.0105 | Validation loss: 0.0070\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Regression(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_features, 128)\n",
    "        self.layer_2 = nn.Linear(128, 128)\n",
    "        self.layer_out = nn.Linear(128, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "input_features = train_ds[0][0].shape[0]\n",
    "model = Regression(input_features=input_features)\n",
    "model.to(pytorch_config.device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model\n",
    "training_loss_h = []\n",
    "val_loss_h = []\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(pytorch_config.epochs):\n",
    "    \"\"\"Trains the model for one epoch.\"\"\"\n",
    "\n",
    "    # Training\n",
    "    training_loss = 0.0\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(pytorch_config.device)\n",
    "        y = y.to(pytorch_config.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = loss_function(outputs, y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "    training_loss /= len(train_loader)\n",
    "    training_loss_h.append(training_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(pytorch_config.device)\n",
    "            y = y.to(pytorch_config.device)\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(X)\n",
    "            loss = loss_function(preds, y)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_loss_h.append(val_loss)\n",
    "\n",
    "    # Print progress\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{pytorch_config.epochs} | Training loss: {training_loss:.4f} | Validation loss: {val_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we are in a kaggle notebook and we want to submit our predictions\n",
    "# to the competition. We can use the test dataset to make predictions and\n",
    "# then submit them to the competition.\n",
    "# Create a PyTorch data loader\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, _ in test_loader:\n",
    "        # Forward pass\n",
    "        X = X.to(pytorch_config.device)\n",
    "        preds = model(X)\n",
    "\n",
    "# Inverse predictions\n",
    "preds = dp.inverse_target_scaler(preds.cpu().numpy())\n",
    "# Round predictions\n",
    "# preds = np.round(preds).astype(int)\n",
    "\n",
    "# Prepare submission\n",
    "# Reload test df\n",
    "# test_df = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "# submission = pd.DataFrame({\"id\": test_df[\"id\"], \"num_sold\": preds.flatten()})\n",
    "# submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1033656489.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    np.round(preds\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
