{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from captum.attr import (\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    IntegratedGradients,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ssl error\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open(\"img/cat.jpg\")\n",
    "test_img_data = np.asarray(test_img)\n",
    "plt.imshow(test_img_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model expects 224x224 3-color image\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "# standard ImageNet normalization\n",
    "transform_normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "transformed_img = transform(test_img)\n",
    "input_img = transform_normalize(transformed_img)\n",
    "input_img = input_img.unsqueeze(0)  # the model requires a dummy batch dimension\n",
    "\n",
    "labels_path = \"img/imagenet_class_index.json\"\n",
    "with open(labels_path) as json_data:\n",
    "    idx_to_labels = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_img)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "pred_label_idx.squeeze_()\n",
    "print(pred_label_idx.item())\n",
    "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "print(\"Predicted:\", predicted_label, \"(\", prediction_score.squeeze().item(), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i[1] for i in idx_to_labels.values()]\n",
    "\n",
    "with open(\"labels.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the attribution algorithm with the model\n",
    "integrated_gradients = IntegratedGradients(model)\n",
    "\n",
    "# Ask the algorithm to attribute our output target to\n",
    "attributions_ig = integrated_gradients.attribute(\n",
    "    input_img, target=pred_label_idx, n_steps=200\n",
    ")\n",
    "\n",
    "# Show the original image for comparison\n",
    "_ = viz.visualize_image_attr(\n",
    "    None,\n",
    "    np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    method=\"original_image\",\n",
    "    title=\"Original Image\",\n",
    ")\n",
    "\n",
    "default_cmap = LinearSegmentedColormap.from_list(\n",
    "    \"custom blue\", [(0, \"#ffffff\"), (0.25, \"#0000ff\"), (1, \"#0000ff\")], N=256\n",
    ")\n",
    "\n",
    "_ = viz.visualize_image_attr(\n",
    "    np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    method=\"heat_map\",\n",
    "    cmap=default_cmap,\n",
    "    show_colorbar=True,\n",
    "    sign=\"positive\",\n",
    "    title=\"Integrated Gradients\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occlusion = Occlusion(model)\n",
    "\n",
    "attributions_occ = occlusion.attribute(\n",
    "    input_img,\n",
    "    target=pred_label_idx,\n",
    "    strides=(3, 8, 8),\n",
    "    sliding_window_shapes=(3, 15, 15),\n",
    "    baselines=0,\n",
    ")\n",
    "\n",
    "\n",
    "_ = viz.visualize_image_attr_multiple(\n",
    "    np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),\n",
    "    [\"original_image\", \"heat_map\", \"heat_map\", \"masked_image\"],\n",
    "    [\"all\", \"positive\", \"negative\", \"positive\"],\n",
    "    show_colorbar=True,\n",
    "    titles=[\"Original\", \"Positive Attribution\", \"Negative Attribution\", \"Masked\"],\n",
    "    fig_size=(18, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_gradcam = LayerGradCam(model, model.layer3[1].conv2)\n",
    "attributions_lgc = layer_gradcam.attribute(input_img, target=pred_label_idx)\n",
    "\n",
    "_ = viz.visualize_image_attr(\n",
    "    attributions_lgc[0].cpu().permute(1, 2, 0).detach().numpy(),\n",
    "    sign=\"all\",\n",
    "    title=\"Layer 3 Block 1 Conv 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsamp_attr_lgc = LayerAttribution.interpolate(attributions_lgc, input_img.shape[2:])\n",
    "\n",
    "print(attributions_lgc.shape)\n",
    "print(upsamp_attr_lgc.shape)\n",
    "print(input_img.shape)\n",
    "\n",
    "_ = viz.visualize_image_attr_multiple(\n",
    "    upsamp_attr_lgc[0].cpu().permute(1, 2, 0).detach().numpy(),\n",
    "    transformed_img.permute(1, 2, 0).numpy(),\n",
    "    [\"original_image\", \"blended_heat_map\", \"masked_image\"],\n",
    "    [\"all\", \"positive\", \"positive\"],\n",
    "    show_colorbar=True,\n",
    "    titles=[\"Original\", \"Positive Attribution\", \"Masked\"],\n",
    "    fig_size=(18, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [\"img/cat.jpg\", \"img/teapot.jpg\", \"img/trilobite.jpg\"]\n",
    "\n",
    "for img in imgs:\n",
    "    img = Image.open(img)\n",
    "    transformed_img = transform(img)\n",
    "    input_img = transform_normalize(transformed_img)\n",
    "    input_img = input_img.unsqueeze(0)  # the model requires a dummy batch dimension\n",
    "\n",
    "    output = model(input_img)\n",
    "    output = F.softmax(output, dim=1)\n",
    "    prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "    pred_label_idx.squeeze_()\n",
    "    predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "    print(\n",
    "        \"Predicted:\",\n",
    "        predicted_label,\n",
    "        \"/\",\n",
    "        pred_label_idx.item(),\n",
    "        \" (\",\n",
    "        prediction_score.squeeze().item(),\n",
    "        \")\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature\n",
    "\n",
    "\n",
    "# Baseline is all-zeros input - this may differ depending on your data\n",
    "def baseline_func(input):\n",
    "    return input * 0\n",
    "\n",
    "\n",
    "# merging our image transforms from above\n",
    "def full_img_transform(input):\n",
    "    i = Image.open(input)\n",
    "    i = transform(i)\n",
    "    i = transform_normalize(i)\n",
    "    i = i.unsqueeze(0)\n",
    "    return i\n",
    "\n",
    "\n",
    "input_imgs = torch.cat(list(map(lambda i: full_img_transform(i), imgs)), 0)\n",
    "\n",
    "visualizer = AttributionVisualizer(\n",
    "    models=[model],\n",
    "    score_func=lambda o: torch.nn.functional.softmax(o, 1),\n",
    "    classes=list(map(lambda k: idx_to_labels[k][1], idx_to_labels.keys())),\n",
    "    features=[\n",
    "        ImageFeature(\n",
    "            \"Photo\",\n",
    "            baseline_transforms=[baseline_func],\n",
    "            input_transforms=[],\n",
    "        )\n",
    "    ],\n",
    "    dataset=[Batch(input_imgs, labels=[282, 849, 69])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
